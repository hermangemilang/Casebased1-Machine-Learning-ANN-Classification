# -*- coding: utf-8 -*-
"""Case-based[1].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aj7k4E311bVBrH1rErvw2LiMxZ1bYIBX
"""

# Commented out IPython magic to ensure Python compatibility.
#import semua library yang dibutuhkan
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score
from keras.callbacks import EarlyStopping
from livelossplot import PlotLossesKeras

#membaca data 
data = pd.read_csv('data_arrhythmia.csv', sep =";")

#Menampilkan 5 data teratas
data.head()

#Menampilkan 5 data terbawah
data.tail()

#menampilkan jumlah baris dan kolom dalam bentuk dua dimensi
data.shape

#statistika data
data.describe()

#menghitung jumlah data yang null
pd.isnull(data).sum().sum()

#mengganti data null dengan lambang "?" menjadi NaN
data = data.replace('?', np.NaN)

#Menampilkan jumlah data null  setelah proses replace
nu=pd.isnull(data).sum().sum()
nu

#menampilkan statistik data yang memiliki nilai null
pd.isnull(data).sum()[7:20].plot()
plt.xlabel('Columns')
plt.ylabel('Total number of null value in each column')

#menampilkan jumlah data null per attribut
pd.isnull(data).sum()[9:16].plot(kind="bar")
plt.xlabel('Columns')
plt.ylabel('Total number of null value in each column')

#menghapus attribut j dan p karena memiliki nilai null paling banyak
data.drop(['J'], inplace=True, axis=1)
data.drop(['P'], inplace=True, axis=1)

#menghitung jumlah null yang terbaru
pd.isnull(data).sum().sum()

#mengecek jumlah atribut setelah proses penghapusan
data.shape

#menghitung jumlah orang per diagnosis level
sns.countplot(x ='diagnosis',data = data) 
plt.show()

#membuat copyan data
new_data = data.copy()

#mengganti nilai data NaN menjadi 0
new_data =new_data.replace(to_replace = np.nan, value = 0)

#menampilkan 5 data teratas
new_data.head()

#menghitung data null setelah replace
pd.isnull(new_data).sum().sum()

#melakukan pengelompokan data untuk proses split
target=new_data["diagnosis"]
final_data = new_data.drop(columns ="diagnosis")

#pengecekan jumlah baris dan kolom
new_data.shape

#menampilkan keselurahan isi new_data
new_data

#melihat korelasi final_data dan diagnosis
target=new_data["diagnosis"]
pearsoncorr = final_data.corrwith(other = target,method='pearson')
pearsoncorr.values

target.replace(to_replace = 2, value = 0,inplace = True)
target.replace(to_replace = 3, value = 0,inplace = True)
target.replace(to_replace = 4, value = 0,inplace = True)
target.replace(to_replace = 5, value = 0,inplace = True)
target.replace(to_replace = 6, value = 0,inplace = True)
target.replace(to_replace = 7, value = 0,inplace = True)
target.replace(to_replace = 8, value = 0,inplace = True)
target.replace(to_replace = 9, value = 0,inplace = True)
target.replace(to_replace = 10, value = 0,inplace = True)
target.replace(to_replace = 11, value = 0,inplace = True)
target.replace(to_replace = 12, value = 0,inplace = True)
target.replace(to_replace = 13, value = 0,inplace = True)
target.replace(to_replace = 14, value = 0,inplace = True)
target.replace(to_replace = 15, value = 0,inplace = True)
target.replace(to_replace = 16, value = 2,inplace = True)

#proses split data train dan testing
X_train,X_test,Y_train,Y_test = train_test_split( final_data, target, test_size=0.2,  random_state = 1)
Y_train

#melihat jumlah kolom dan baris setelah proses split
print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)

#normalisasi data
sc = StandardScaler()
sc.fit(X_train)
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
print(X_train)

#proses intialisasi data

model = tf.keras.models.Sequential()

#add a input layer
model.add(keras.layers.Dense(units=277, activation='relu', input_shape=X_train[0].shape))

#Add a hidden layer
model.add(keras.layers.Dense(units = 12,kernel_initializer ='glorot_uniform',name='Hidden-Layer-1',activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(keras.layers.Dense(units = 9,kernel_initializer ='glorot_uniform',name='Hidden-Layer-2',activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
#Add a output layer
model.add(keras.layers.Dense(units = 1,kernel_initializer ='glorot_uniform',activation='sigmoid'))

# compile the keras model/hyperparameter
model.compile(optimizer=tf.keras.optimizers.Adamax(0.001), loss='binary_crossentropy',metrics=['accuracy'])

monitor_val_acc = EarlyStopping(monitor = 'val_loss', patience = 5)
callbacks=[PlotLossesKeras()] 



#train data/ proses validasi
model.fit(X_train,Y_train, validation_split= 0.15, epochs=200, shuffle=True, batch_size = 32, callbacks=[PlotLossesKeras(), monitor_val_acc], verbose=0)

#evaluasi  train keras model
_, accuracy = model.evaluate(X_train, Y_train)
print('Accuracy: %.2f' % (accuracy*100))

#melakukan prediksi berdasarkan data test
y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5)

pd.DataFrame(list(zip(Y_test,y_pred)), columns=['Actual', 'Predicted'])

#menampilkan matriks akurasi data testing
print(confusion_matrix(Y_test, y_pred))
print("akurasi : ",accuracy_score(Y_test, y_pred)*100,'%')